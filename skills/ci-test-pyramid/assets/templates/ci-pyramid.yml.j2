# Test Pyramid CI Workflow
# Generated by ci-test-pyramid skill

name: Test Pyramid

on:
  push:
    branches: [{{ config.deploy.get('main_branch', 'main') }}, develop]
  pull_request:
    branches: [{{ config.deploy.get('main_branch', 'main') }}]

env:
  PYTHON_VERSION: "{{ python_version | default('3.11') }}"
  MINIMUM_PYRAMID_SCORE: "{{ config.test_pyramid.get('minimum_score', 5.5) }}"

jobs:
  test-pyramid-check:
    name: Pyramid Health Check
    runs-on: ubuntu-latest
    outputs:
      score: ${{ steps.pyramid.outputs.score }}
      passed: ${{ steps.pyramid.outputs.passed }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest

      - name: Run Pyramid Monitor
        id: pyramid
        run: |
          python scripts/testing/pyramid-monitor.py --format json > pyramid-report.json
          SCORE=$(jq '.score' pyramid-report.json)
          PASSED=$(jq '.passed' pyramid-report.json)
          echo "score=$SCORE" >> $GITHUB_OUTPUT
          echo "passed=$PASSED" >> $GITHUB_OUTPUT
          echo "## Test Pyramid Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Score:** $SCORE/10.0" >> $GITHUB_STEP_SUMMARY
          python scripts/testing/pyramid-monitor.py --format markdown >> $GITHUB_STEP_SUMMARY

      - name: Check Score Threshold
        run: |
          SCORE=$(jq '.score' pyramid-report.json)
          if (( $(echo "$SCORE < $MINIMUM_PYRAMID_SCORE" | bc -l) )); then
            echo "::error::Test pyramid score too low: $SCORE < $MINIMUM_PYRAMID_SCORE"
            exit 1
          fi
          echo "Pyramid score: $SCORE >= $MINIMUM_PYRAMID_SCORE ✓"

      - name: Upload Report
        uses: actions/upload-artifact@v4
        with:
          name: pyramid-report
          path: pyramid-report.json

  validate-markers:
    name: Validate Test Markers
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Validate Markers
        run: |
          python scripts/testing/validate-markers.py --all --verbose

{% if config.test_pyramid.get('pr_comments', true) %}
  comment-pr:
    name: Comment PR with Results
    runs-on: ubuntu-latest
    needs: [test-pyramid-check]
    if: github.event_name == 'pull_request'

    steps:
      - name: Download Report
        uses: actions/download-artifact@v4
        with:
          name: pyramid-report

      - name: Comment on PR
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = JSON.parse(fs.readFileSync('pyramid-report.json', 'utf8'));

            const body = `## Test Pyramid Report

            **Score:** ${report.score}/10.0 ${report.passed ? '✅' : '❌'}
            **Minimum Required:** ${{ env.MINIMUM_PYRAMID_SCORE }}

            | Type | Count | Percentage | Target | Delta |
            |------|-------|------------|--------|-------|
            | Unit | ${report.distribution.unit.count} | ${report.distribution.unit.percentage}% | ${report.distribution.unit.target}% | ${report.distribution.unit.delta > 0 ? '+' : ''}${report.distribution.unit.delta}% |
            | Integration | ${report.distribution.integration.count} | ${report.distribution.integration.percentage}% | ${report.distribution.integration.target}% | ${report.distribution.integration.delta > 0 ? '+' : ''}${report.distribution.integration.delta}% |
            | E2E | ${report.distribution.e2e.count} | ${report.distribution.e2e.percentage}% | ${report.distribution.e2e.target}% | ${report.distribution.e2e.delta > 0 ? '+' : ''}${report.distribution.e2e.delta}% |

            **Total Tests:** ${report.total_tests}
            ${report.missing_markers > 0 ? `**⚠️ Uncategorized:** ${report.missing_markers} tests need markers` : ''}
            `;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body
            });
{% endif %}

  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    needs: [test-pyramid-check, validate-markers]

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-dev.txt 2>/dev/null || true

      - name: Run Unit Tests
        run: |
          pytest -m unit --junitxml=junit-unit.xml -v

      - name: Upload Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: unit-test-results
          path: junit-unit.xml

  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: [test-pyramid-check, validate-markers]

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-dev.txt 2>/dev/null || true

      - name: Run Integration Tests
        run: |
          pytest -m integration --junitxml=junit-integration.xml -v

      - name: Upload Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: integration-test-results
          path: junit-integration.xml

  e2e-tests:
    name: E2E Tests
    runs-on: ubuntu-latest
    needs: [test-pyramid-check, validate-markers]
    timeout-minutes: 30

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-dev.txt 2>/dev/null || true

      - name: Run E2E Tests
        run: |
          pytest -m e2e --junitxml=junit-e2e.xml -v

      - name: Upload Results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: e2e-test-results
          path: junit-e2e.xml
